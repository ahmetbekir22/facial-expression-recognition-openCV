{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 15:47:54.065 Python[97065:5807676] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-03-17 15:47:54.065 Python[97065:5807676] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Kamerayƒ± ba≈ülat\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    cv2.imshow(\"Webcam\", frame)\n",
    "\n",
    "    # 'q' tu≈üuna basƒ±nca √ßƒ±k\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1742215748.520855 5807676 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1742215748.534872 5809655 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742215748.539512 5809655 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742215749.887300 5809657 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFace Mesh\u001b[39m\u001b[38;5;124m\"\u001b[39m, frame)\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# 'q' tu≈üuna basƒ±nca √ßƒ±k\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     39\u001b[0m cap\u001b[38;5;241m.\u001b[39mrelease()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Y√ºz tespiti i√ßin MediaPipe modelini y√ºkle\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1)\n",
    "\n",
    "# Kamerayƒ± ba≈ülat\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # OpenCV'nin BGR formatƒ±nƒ± RGB'ye √ßevir\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # MediaPipe ile y√ºz analizi yap\n",
    "    results = face_mesh.process(rgb_frame)\n",
    "\n",
    "    # Eƒüer y√ºz algƒ±lanƒ±rsa √ßizim yap\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            for landmark in face_landmarks.landmark:\n",
    "                # Pixel koordinatlarƒ±nƒ± al\n",
    "                x = int(landmark.x * frame.shape[1])\n",
    "                y = int(landmark.y * frame.shape[0])\n",
    "                \n",
    "                # Y√ºz noktalarƒ±nƒ± √ßiz\n",
    "                cv2.circle(frame, (x, y), 1, (0, 255, 0), -1)\n",
    "\n",
    "    cv2.imshow(\"Face Mesh\", frame)\n",
    "\n",
    "    # 'q' tu≈üuna basƒ±nca √ßƒ±k\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Emoji eklemek i√ßin Unicode karakterleri\n",
    "EMOJI_MAP = {\n",
    "    \"neutral\": \"üòê\",\n",
    "    \"happy\": \"üòÉ\",\n",
    "    \"sad\": \"üò¢\",\n",
    "    \"surprised\": \"üò≤\"\n",
    "}\n",
    "\n",
    "# MediaPipe y√ºz tespiti ba≈ülat\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1)\n",
    "\n",
    "# Kamerayƒ± a√ß\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # OpenCV'den RGB'ye √ßevir\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(rgb_frame)\n",
    "\n",
    "    # Varsayƒ±lan olarak neutral emoji\n",
    "    current_emoji = EMOJI_MAP[\"neutral\"]\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            landmarks = face_landmarks.landmark\n",
    "            \n",
    "            # Aƒüƒ±z a√ßƒ±klƒ±ƒüƒ±nƒ± hesapla (√ºst dudak ve alt dudak mesafesi)\n",
    "            top_lip = np.array([landmarks[13].x, landmarks[13].y])\n",
    "            bottom_lip = np.array([landmarks[14].x, landmarks[14].y])\n",
    "            mouth_distance = np.linalg.norm(top_lip - bottom_lip)\n",
    "\n",
    "            # Ka≈ü y√ºksekliƒüini hesapla (ka≈ülar ile g√∂zler arasƒ± mesafe)\n",
    "            left_brow = np.array([landmarks[55].x, landmarks[55].y])\n",
    "            left_eye = np.array([landmarks[33].x, landmarks[33].y])\n",
    "            brow_distance = np.linalg.norm(left_brow - left_eye)\n",
    "\n",
    "            # Mutluluk tespiti (aƒüƒ±z √ßok a√ßƒ±k => mutlu)\n",
    "            if mouth_distance > 0.05:\n",
    "                current_emoji = EMOJI_MAP[\"happy\"]\n",
    "            # √úz√ºnt√º tespiti (ka≈ülar √ßok a≈üaƒüƒ±da)\n",
    "            elif brow_distance < 0.02:\n",
    "                current_emoji = EMOJI_MAP[\"sad\"]\n",
    "            # ≈ûa≈ükƒ±nlƒ±k tespiti (aƒüƒ±z √ßok fazla a√ßƒ±k)\n",
    "            elif mouth_distance > 0.08:\n",
    "                current_emoji = EMOJI_MAP[\"surprised\"]\n",
    "\n",
    "    # Emoji'yi ekrana bas\n",
    "    cv2.putText(frame, current_emoji, (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 3)\n",
    "\n",
    "    cv2.imshow(\"Emoji Face\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
